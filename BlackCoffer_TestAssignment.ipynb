{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTmQ0fM73ewy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows',None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tDVDQccPLxHM"
      },
      "outputs": [],
      "source": [
        "excel_data=pd.read_excel('Input.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "01YyEC-_qLn1"
      },
      "outputs": [],
      "source": [
        "excel_data.drop([48,35], inplace=True)#Error 404 on these websites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hpQ52CZNNQf7"
      },
      "outputs": [],
      "source": [
        "def Parsing(url):  #Parsing the pages\n",
        "    urls=[]\n",
        "    url=excel_data['URL']\n",
        "\n",
        "    for _ in url:\n",
        "        urls.append(_)\n",
        "\n",
        "    responses=[]\n",
        "    for link in urls:\n",
        "        response=requests.get(link)\n",
        "        response.raise_for_status()\n",
        "        responses.append(response)\n",
        "\n",
        "    soups=[]\n",
        "    for r in responses:\n",
        "        soup=BeautifulSoup(r.content,'html.parser')\n",
        "        soups.append(soup)\n",
        "    return soups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BDMuDwIFWlKV"
      },
      "outputs": [],
      "source": [
        "def extract(soups):\n",
        "    all_text = []\n",
        "    titles = []\n",
        "\n",
        "    for soup in soups:\n",
        "        heads = soup.find('h1', class_=\"entry-title\")\n",
        "        texts = soup.find('div', class_=\"td-post-content tagdiv-type\")\n",
        "\n",
        "        if heads is not None and texts is not None:\n",
        "            heads = heads.text.strip()\n",
        "            text = texts.text.strip()\n",
        "            all_text.append(text)\n",
        "            titles.append(heads)\n",
        "        else:\n",
        "            heads = soup.find('h1', class_=\"tdb-title-text\").text.strip()\n",
        "            texts = soup.find_all('div', class_='tdb-block-inner td-fix-index')\n",
        "\n",
        "            paragraphs_list = []\n",
        "            for t in texts:\n",
        "                paragraphs = t.find_all('p')\n",
        "\n",
        "                for paragraph in paragraphs:\n",
        "                    paragraphs_list.append(paragraph.text.strip())\n",
        "\n",
        "            text = ' '.join(paragraphs_list)\n",
        "            all_text.append(text)\n",
        "            titles.append(heads)\n",
        "\n",
        "    data = {'Title': titles, 'Text': all_text}\n",
        "    dataframe = pd.DataFrame(data)\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdnp4L5JWmSc"
      },
      "outputs": [],
      "source": [
        "df = extract(Parsing(excel_data['URL']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gvUsntMn3uZU"
      },
      "outputs": [],
      "source": [
        "def clean(text):  #Function to clean the text by removing all special characters and numericals leaving the alphabets\n",
        "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
        "    return text\n",
        "\n",
        "df['Cleaned Text'] = df['Text'].apply(clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wLa8YC07ksXx"
      },
      "outputs": [],
      "source": [
        "stopwords = [\n",
        "    'StopWords/StopWords_Currencies.txt',\n",
        "    'StopWords/StopWords_DatesandNumbers.txt',\n",
        "    'StopWords/StopWords_Generic.txt',\n",
        "    'StopWords/StopWords_GenericLong.txt',\n",
        "    'StopWords/StopWords_Geographic.txt',\n",
        "    'StopWords/StopWords_Names.txt'\n",
        "]\n",
        "sw_list=[]\n",
        "for stopword in stopwords:\n",
        "  with open(stopword, 'r', encoding='latin=1') as file:\n",
        "        content = file.read()\n",
        "        sw_list.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeLa9AkaTlbo"
      },
      "outputs": [],
      "source": [
        "cleaned_sw_list = [clean(text) for text in sw_list]\n",
        "result_list = [] #Final cleaned list containing all the stopwords\n",
        "\n",
        "for item in cleaned_sw_list:: #Seperating each word\n",
        "    words = item.split()\n",
        "    result_list.extend(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XGkBojtBRwbi"
      },
      "outputs": [],
      "source": [
        "def clean_stopword(data, stop_words):\n",
        "    if isinstance(data, str):\n",
        "        # If input is a string, split it into a list of words\n",
        "        words = data.split()\n",
        "    elif isinstance(data, list):\n",
        "        # If input is a list, use it directly\n",
        "        words = data\n",
        "    else:\n",
        "        raise ValueError(\"Input data must be either a string or a list\")\n",
        "\n",
        "    # Filter out stop words\n",
        "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    if isinstance(data, str):\n",
        "        # If input was a string, join the cleaned words into a string\n",
        "        cleaned_data = ' '.join(cleaned_words)\n",
        "    else:\n",
        "        # If input was a list, return the cleaned list\n",
        "        cleaned_data = cleaned_words\n",
        "\n",
        "    return cleaned_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myZB-DUlH7Eq"
      },
      "outputs": [],
      "source": [
        "df['Cleaned Text'] = df['Text'].apply(clean_stopword,stop_words=result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnhMq4i6-TUi"
      },
      "outputs": [],
      "source": [
        "positive='MasterDictionary/positive-words.txt'\n",
        "positive_words=[]\n",
        "PositiveWord=[]\n",
        "with open(positive, 'r', encoding='latin-1') as file:\n",
        "  positive_word=file.read()\n",
        "  positive_words.append(positive_word)\n",
        "  cleaned_positive_words = [clean(text) for text in positive_words]\n",
        "\n",
        "  for item in cleaned_positive_words:\n",
        "    Word=item.split()\n",
        "    PositiveWord.extend(Word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSyNgTCr-TZE"
      },
      "outputs": [],
      "source": [
        "negative='MasterDictionary/negative-words.txt'\n",
        "negative_words=[]\n",
        "negativeWord=[]\n",
        "with open(negative, 'r', encoding='latin-1') as file:\n",
        "  negative_word=file.read()\n",
        "  negative_words.append(negative_word)\n",
        "  cleaned_negative_words = [clean(text) for text in negative_words]\n",
        "\n",
        "  for item in cleaned_negative_words:\n",
        "    Word=item.split()\n",
        "    negativeWord.extend(Word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QvruwzCMbkZ"
      },
      "outputs": [],
      "source": [
        "cleaned_PositiveWords=clean_stopword(PositiveWord,result_list)\n",
        "cleaned_NegativeWords=clean_stopword(negativeWord,result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dPvaIdnJTVFr"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "df['Tokenized Text'] = df['Cleaned Text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BIfZDG5YSWk"
      },
      "outputs": [],
      "source": [
        "Positive_score = []\n",
        "Negative_score = []\n",
        "Polarity_score = []\n",
        "Subjectivity_score = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    words = row['Tokenized Text']\n",
        "    positivescore = 0\n",
        "    negativescore = 0\n",
        "    totalwords = 0\n",
        "\n",
        "    for word in words:\n",
        "        if word in cleaned_PositiveWords:\n",
        "            positivescore += 1\n",
        "            totalwords += 1\n",
        "        elif word in cleaned_NegativeWords:\n",
        "            negativescore -= 1*(- 1)\n",
        "            totalwords += 1\n",
        "\n",
        "    Negative_score.append(negativescore)\n",
        "    Positive_score.append(positivescore)\n",
        "\n",
        "    # Calculate polarity and subjectivity scores\n",
        "    polarityscore = (positivescore - negativescore) / ((positivescore + negativescore) + 0.000001)\n",
        "    subjectivityscore = (positivescore + negativescore) / (totalwords + 0.000001)\n",
        "\n",
        "    Polarity_score.append(polarityscore)\n",
        "    Subjectivity_score.append(subjectivityscore)\n",
        "\n",
        "df['Positive Score'] = Positive_score\n",
        "df['Negative Score'] = Negative_score\n",
        "df['Polarity Score'] = Polarity_score\n",
        "df['Subjectivity Score'] = Subjectivity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTolQwz8aYv8"
      },
      "outputs": [],
      "source": [
        "from textstat import sentence_count, lexicon_count, syllable_count\n",
        "\n",
        "def calculate_gunning_fog(text):\n",
        "\n",
        "    average_sentence_length = lexicon_count(text) / sentence_count(text)\n",
        "\n",
        "    complex_words = [word for word in text.split() if syllable_count(word) >= 3]\n",
        "    percentage_complex_words = len(complex_words) / lexicon_count(text)\n",
        "\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    return fog_index, average_sentence_length, percentage_complex_words\n",
        "\n",
        "df[['Fog Index', 'Average Sentence Length', 'Percentage of Complex Words']] = df['Text'].apply(calculate_gunning_fog).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZT28171haY5B"
      },
      "outputs": [],
      "source": [
        "def avg_words_per_sentence(text):\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "    words_count = sum(len(sentence.split()) for sentence in sentences if sentence.strip())\n",
        "    sentences_count = len(sentences)\n",
        "    return words_count / sentences_count if sentences_count else 0\n",
        "\n",
        "def complex_word_count(text):\n",
        "    return len([word for word in text.split() if syllable_count(word) >= 3])\n",
        "\n",
        "def personal_pronouns_count(text):\n",
        "    pronouns = ['I', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves']\n",
        "    return sum(text.split().count(pronoun) for pronoun in pronouns)\n",
        "\n",
        "def avg_word_length(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return sum(len(word) for word in words) / len(words) if len(words) else 0\n",
        "\n",
        "df['Avg Words per Sentence'] = df['Text'].apply(avg_words_per_sentence)\n",
        "df['Complex Word Count'] = df['Text'].apply(complex_word_count)\n",
        "df['Personal Pronouns Count'] = df['Text'].apply(personal_pronouns_count)\n",
        "df['Avg Word Length'] = df['Text'].apply(avg_word_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "GUn1jJnfQARF"
      },
      "outputs": [],
      "source": [
        "def calculate_word_count(text):\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def calculate_syllables_per_word(text):\n",
        "    words = text.split()\n",
        "    total_syllables = sum(syllable_count(word) for word in words)\n",
        "    if len(words) > 0:\n",
        "        return total_syllables / len(words)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['Word Count'] = df['Text'].apply(calculate_word_count)\n",
        "df['Syllables per Word'] = df['Text'].apply(calculate_syllables_per_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CSve7x73O37R"
      },
      "outputs": [],
      "source": [
        "new_df=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ePBNs1mjOlSS"
      },
      "outputs": [],
      "source": [
        "new_df.drop(['Title','Text','Cleaned Text','Tokenized Text'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1TuiF6NO7Ui"
      },
      "outputs": [],
      "source": [
        "merged_data = pd.concat([excel_data, new_df], axis=1)\n",
        "new_df=merged_data\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sSyC5kqVgiB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "new_df.to_excel('Output.xlsx', encoding = 'utf-8-sig')\n",
        "files.download('Output.xlsx')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
